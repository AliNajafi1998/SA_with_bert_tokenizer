{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SA_bert_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_PW-nhPRG4z",
        "outputId": "634b06f5-4297-4bae-d188-28f7bd18d525"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFMNnw6oRPWV",
        "outputId": "4cbc29ce-28a6-45c8-b84e-2884759c04bb"
      },
      "source": [
        "%cd \"drive/MyDrive/Projects/BERT/BERT_TOKENIZER\"\n",
        "# !wget \"cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"\n",
        "# !unzip trainingandtestdata.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Projects/BERT/BERT_TOKENIZER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMH3vSM_wAFb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GZC_1VHQ5iR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfhums_lSiXB",
        "outputId": "9d9aed54-cfd9-49c6-f92e-c335586772a6"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/820ccaf55f1e24b5dd43583ac0da6d86c2d27bbdfffadbba69bafe73ca93/bert-for-tf2-0.14.7.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.7-cp36-none-any.whl size=30537 sha256=43fefdfa6a343d545fdcfb7cae2f7cec8db42992a97812a84f3f2f2c4ab57b48\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/f8/e2/b98f79a6b8cc898d8e4102b83acb8a098df7d27500a2bac912\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7303 sha256=16e719e45c7d76909114f087a7d65f004fbfa787e4f2752e8d4d16faa9079490\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19474 sha256=04dee7ca768ab7bdb09ac8312d9621796a45d26a036ec236669a499802285cbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.7 params-flow-0.8.2 py-params-0.9.7\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 10.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oInxhFZLUM2m"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF74fpOWW5ts"
      },
      "source": [
        "columns = [\"sentiment\",\"id\",\"date\",\"query\",\"user\",\"text\"]\n",
        "data = pd.read_csv(\n",
        "                \"train.csv\",\n",
        "                header=None,\n",
        "                names=columns,\n",
        "                engine=\"python\",\n",
        "                encoding=\"latin1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D64lBWLcZcvd"
      },
      "source": [
        "data.drop([\"id\",\"date\",\"query\",\"user\"],axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQceq_hBgDur"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet,'lxml').get_text()\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\",' ',tweet)\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\",\" \",tweet)\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\",\" \",tweet)\n",
        "    tweet = re.sub(r\" +\",' ',tweet)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyF1y7ZxivWR"
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data['text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyb7n-yBjA2W"
      },
      "source": [
        "data_labels = data[\"sentiment\"].values\n",
        "data_labels[data_labels == 4] = 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUuJM_DiZuKs"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "\n",
        "vocab_file =  bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()    \n",
        "tokenizer = FullTokenizer(vocab_file,do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BFq2EjfaWyA"
      },
      "source": [
        "def encode_sentences(sentence):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM5JNq0-aW0s"
      },
      "source": [
        "data_inputs = [encode_sentences(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4hsA-2Pi8tc"
      },
      "source": [
        "data_with_len = [ [sent,data_labels[i],len(sent)] for i,sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x : x[2])\n",
        "sorted_all = [ (sent_lab[0],sent_lab[1]) for sent_lab in data_with_len\n",
        "              if sent_lab[2] > 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtSndv96oPZj"
      },
      "source": [
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,output_types=(tf.int32,tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DYH0HEioPc0"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(\n",
        "                batch_size=BATCH_SIZE,\n",
        "                padded_shapes=((None,),())\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a2LaTDooPf8"
      },
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all)/BATCH_SIZE)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_data_set = all_batched.take(NB_BATCHES_TEST)\n",
        "train_data_set = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpePhlD9oPit"
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name='dcnn'):\n",
        "        super(DCNN,self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size,emb_dim)\n",
        "        \n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding='valid',\n",
        "                                    activation='relu')\n",
        "\n",
        "        self.trigram =  layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=3,\n",
        "                                      padding='valid',\n",
        "                                      activation='relu')\n",
        "        \n",
        "        self.fourgram =  layers.Conv1D(filters=nb_filters,\n",
        "                                       kernel_size=4,\n",
        "                                       padding='valid',\n",
        "                                       activation='relu')\n",
        "        \n",
        "        self.pool = layers.GlobalMaxPool1D()     \n",
        "\n",
        "        self.dense_1 = layers.Dense(units=FFN_units,\n",
        "                                    activation='relu')\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,activation='sigmoid')\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,activation='softmax')\n",
        "\n",
        "    def call(self,inputs,training):\n",
        "        x = self.embedding(inputs)\n",
        "\n",
        "        x_1 = self.bigram(x)\n",
        "        x_1 = self.pool(x_1) # (batch_size,nb_filters)\n",
        "\n",
        "        x_2 = self.trigram(x)\n",
        "        x_2 = self.pool(x_2) # (batch_size,nb_filters)\n",
        "\n",
        "        x_3 = self.fourgram(x)\n",
        "        x_3 = self.pool(x_3) # (batch_size,nb_filters)\n",
        "\n",
        "        merged = layers.concatenate([x_1,x_2,x_3],axis=-1) # (batch_size,3*nb_filters)\n",
        "\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged,training=training)\n",
        "        \n",
        "        output = self.last_dense(merged)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvsxsdfpoPlM"
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "DROPOUT_RATE = 0.2\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlKZi7EfoPn6"
      },
      "source": [
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W68XCfm1MMZ"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "     Dcnn.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])    \n",
        "else:\n",
        "     Dcnn.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['sparse_categorical_accuracy'])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3BYhlby1s3v"
      },
      "source": [
        "checkpoint_path = \"ckpt\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint_path,max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest chekpoint has been restored!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFRS1vS72mSK"
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def on_epoch_end(self,epoch,logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(f\"Checkpoint saved at {checkpoint_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jusYL0lv3GK7",
        "outputId": "0d79c924-1a6e-42ff-f167-2f09462502c8"
      },
      "source": [
        "Dcnn.fit(train_data_set,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "44240/44240 [==============================] - 2674s 60ms/step - loss: 0.3978 - accuracy: 0.8208\n",
            "Checkpoint saved at ckpt\n",
            "Epoch 2/5\n",
            "44240/44240 [==============================] - 2653s 60ms/step - loss: 0.3427 - accuracy: 0.8526\n",
            "Checkpoint saved at ckpt\n",
            "Epoch 3/5\n",
            "44240/44240 [==============================] - 2660s 60ms/step - loss: 0.3125 - accuracy: 0.8682\n",
            "Checkpoint saved at ckpt\n",
            "Epoch 4/5\n",
            "44240/44240 [==============================] - 2667s 60ms/step - loss: 0.2847 - accuracy: 0.8819\n",
            "Checkpoint saved at ckpt\n",
            "Epoch 5/5\n",
            "44240/44240 [==============================] - 2629s 59ms/step - loss: 0.2589 - accuracy: 0.8935\n",
            "Checkpoint saved at ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbb328cdeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiI1RJVG3Yjk"
      },
      "source": [
        "def predict(example_sent):\n",
        "    example_sent = encode_sentences(example_sent)\n",
        "    example_sent = np.expand_dims(example_sent,axis=0)\n",
        "    prediction = Dcnn.predict(example_sent)\n",
        "    prediction = int(np.floor(prediction*2)[0,0])\n",
        "    if prediction == 1:\n",
        "        print(\"positive\")\n",
        "    else:\n",
        "        print(\"negative\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5Q2YVokuewE",
        "outputId": "c7aa1013-60ce-4bc5-b333-373c173e7d30"
      },
      "source": [
        "negative_example = \"I am so sad\"\n",
        "predict(negative_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8DQ9ac5vjYn",
        "outputId": "579bd291-b84e-4e5e-ff29-f2566e224e5a"
      },
      "source": [
        "positive_example = \"He is my best friend\"\n",
        "predict(positive_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1_5oNXlvwM5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}